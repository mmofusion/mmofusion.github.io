<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model.">
  <meta name="keywords" content="Motion Generation, Multi-Modal, Diffusion">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
            Sen Wang<sup>1</sup>,</span>
            <span class="author-block">
            Jiangning Zhang<sup>3</sup>,</span>
            <span class="author-block">
            Weijian Cao<sup>3</sup>,
            </span>
            <span class="author-block">
            Xiaobin Hu<sup>3</sup>,
            </span>
            <span class="author-block">
            Moran Li<sup>3</sup>,
            </span>
            <span class="author-block">
            Xiaozhong Ji<sup>3</sup>,
            </span>
            <span class="author-block">
            Xiaozhong Ji<sup>3</sup>,
            </span>
            <span class="author-block">
            Xin Tan<sup>4</sup>,
            </span>
            <span class="author-block">
            Mengtian Li<sup>1,2</sup>,
            </span>
            <span class="author-block">
            Zhifeng Xie<sup>1,2</sup>,
            </span>
            <span class="author-block">
            Chengjie Wang<sup>3,5</sup>,
            </span>
            <span class="author-block">
            Lizhuang Ma<sup>4,5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Engineering Research Center of Motion Picture Special Effects,</span>
            <span class="author-block"><sup>3</sup>Tencent Youtu Lab,</span>
            <span class="author-block"><sup>4</sup>East China Normal University,</span>
            <span class="author-block"><sup>5</sup>Shanghai Jiao Tong University,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.02905"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/intro.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>

      <h2 class="subtitle has-text-centered">
        Multi-modal input for co-speech motion generation.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/4_our.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/13_our.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/1_our_upper.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/27_our_upper.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The body movements accompanying speech aid speakers in expressing their ideas.
            Co-speech motion generation is one of the important approaches for synthesizing realistic avatars. 
            Due to the intricate correspondence between speech and motion, generating realistic and diverse motion is a challenging task. 
          </p>
          <p>
            In this paper, we propose <b>MMoFusion</b>, a <b>M</b>ulti-modal co-speech <b>Mo</b>tion generation framework based on dif<b>Fusion</b> model to ensure both the authenticity and diversity of generated motion.
            We propose a progressive fusion strategy to enhance the interaction of inter-modal and intra-modal, efficiently integrating multi-modal information. 
            Specifically, we employ a masked style matrix based on emotion and identity information to control the generation of different motion styles. 
            Temporal modeling of speech and motion is partitioned into style-guided specific feature encoding and shared feature encoding, aiming to learn both inter-modal and intra-modal features.
          </p>
          <p>
            Besides, we propose a geometric loss to enforce the joints' velocity and acceleration coherence among frames.
            Our framework generates vivid, diverse, and style-controllable motion of arbitrary length through inputting speech and editing identity and emotion. 
            Extensive experiments demonstrate that our method outperforms current co-speech motion generation methods including upper body and challenging full body.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <h3 class="title is-4">Compare with other methods</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
          controls
          muted
          preload
          playsinline
          width="75%">
              <source src="./static/videos/com.mp4"
                type="video/mp4">
          </video>

        </div>
        <br/>

        <h3 class="title is-4">Style Control</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/style.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Custom Speech</h3>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/custom.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{wang2024mmofusion,
      title={MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model}, 
      author={Sen Wang and Jiangning Zhang and Weijian Cao and Xiaobin Hu and Moran Li and Xiaozhong Ji and Xin Tan and Mengtian Li and Zhifeng Xie and Chengjie Wang and Lizhuang Ma},
      year={2024},
      eprint={2403.02905},
      archivePrefix={arXiv},
      primaryClass={cs.MM}
}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
